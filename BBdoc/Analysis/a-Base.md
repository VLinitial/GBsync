# BlueBase

## Introduction to BlueBase

BlueBase is a **genomics data aggregation** and **knowledge management** solution suite.
It is a secure and scalable integrated genomics **data analysis solution** which provides Information management and knowledge mining.
Users are able to **analyze, aggregate and query data** for new insights that can inform and improve diagnostic assay development, clinical trials, patient testing and patient care. For this, all clinically relevant data generated from routine clinical testing needs to be extracted and clinical questions need to be asked across all data and information sources.
As a **large data store**, BlueBase provides a secure and compliant environment to accumulate data, allowing for efficient exploration of the aggregated data. This data consists of test results, patient data, metadata, reference data, consent and QC data.

<!--PM: NOTE: I think "possible usage" should come before actions from a flow perspective since it's higher-level -->
<!--PM: NOTE: edited some grammar/syntax -->

### BlueBase User Personas and Use Cases

BlueBase can be used by different user personas supporting different use cases:

* Clinical and Academic Researchers:
  * Big data storage solution housing all aggregated sample test outcomes
  * Analyze information by way of a convenient query formalism
  * Look for signals in combined phenotypic and genotypic data
  * Analyze QC patterns over large cohorts of patients
  * Securely share (sub)sets of data with other scientists
  * Generate reports and analyze trends in a straightforward and simple manner.
* Bioinformaticians:
  * Access, consult, audit, and query all relevant data and QC information for tests run
  * All accumulated data and accessible pipelines can be used to investigate and improve bioinformatics for clinical analysis
  * Metadata is captured via automatic pipeline version tracking, including information on individual tools and/or reference files used during processing for each sample analyzed, information on the duration of the pipeline, the execution path of the different analytical steps, or in case of failure, exit codes can be warehoused.
* Product Developers and Service Providers:
  * Better understand the efficiency of kits and tests
  * Analyze usage, understand QC data trends, improve products
  * Store and aggregate business intelligence data such as lab identification, consumption patterns and frequency, as well as allow renderings of test result outcome trends and much more.

### BlueBase Action Possibilities

* **Data Warehouse Creation**: in which desired data sets can be selected and aggregated. Typical data sets include available VCF and other suitable (meta)data files generated by the BlueBee platform which can be complemented by additional public (or privately built) databases.
* **Report and Export**: Once created, a data warehouse can be mined using standard database query instructions. All BlueBase data is stored in a structured and easily accessible way. An interface allows for the selection of specific datasets and conditional reporting. All queries can be stored, shared, and re-used in the future. This type of standard functionality supports most expected basic mining operations, such as variant frequency aggregation. All result sets can be downloaded or exported in various standard data formats for integration in other reporting or analytical applications.
* **Detect Signals and Patterns**: extensive and detailed selection of subsets of patients or samples adhering to any imaginable set of conditions is possible. Users can, for example, group and list subjects based on a combination of (several) specific genetic variants in combination with patient characteristics such as therapeutic (outcome) information. The built-in integration with public datasets allows users to retrieve all relevant publications, or clinically significant information for a single individual or a group of samples with a specific variant. Virtually any possible combination of stored sample and patient information allow for detecting signals and patterns by a simple single query on the big data set.
* **Profile/Cluster patients**: use and re-analyze patient cohort information based on specific sample or individual characteristics. For instance, they might want to run a next agile iteration of clinical trials with only patients that respond. Through integrated and structured consent information allowing for time-boxed use, combined with the capability to group subjects by the use of a simple query, patients can be stratified and combined to export all relevant individuals with their genotypic and phenotypic information to be used for further research.
* **Share your data**: Data sharing is subject to strict ethical and regulatory requirements. BlueBase provides built-in functionality to securely share (sub)sets of your aggregated data with third parties. All data access can be monitored and audited, in this way BlueBase data can be shared with people in and outside of an organization in a compliant and controlled fashion.

## Access

BlueBase is a module that can be found in a project. It is shown in the menu bar of the project.

> ❗️Before users can access BlueBase:
>
> * On the domain level, BlueBase needs to be included in the subscription
> * On the project level, the project owner needs to enable BlueBase
> * On the user level, the project administrator needs to enable workgroups to access the BlueBase pages

### Permission to enable BlueBase

<!--PM: QUESTION: - I'll try to re-word this, but first, I don't understand this instruction. It states an activation code is needed but then states the activation code is granted automatically upon account creation. This implies the account code is automatically granted, so does this mean it's automatically enabling bluebase, thereby rendering this instruction pointless? Or does the activation code live somewhere else?
ANWSER: The activation code is a BlueBee Concept and not visible to the user. It is generated once the subscription is recognized by BB when the domain is first configured during the first user login. This does not automatically activate the BlueBase module. It simply makes activation possible by the Project Owner via the "enable" button.-->

The access to activate the BlueBase module is controlled based upon the chosen subscription (full and premium subscriptions give access to BlueBase) when registering the account.
This will all happen automatically after the first user logs into the system for that account. So from the moment the account is up and running, the BlueBase module will also be ready to be enabled.

### Enable BlueBase

When a user has created a project, they can go to the BlueBase pages and click the **Enable** button. From that moment on, every user who has the proper permissions has access to the BlueBase module in that project.

### Access BlueBase pages

Access to the projects and all modules located within the project is provided via the **Team** page within the project.

<!-- Access to the BlueBase Management page is provided.. TODO Complete this section; Check for accuracy - @Vicky: Is this still available on the new platform? + which users (permissions) can access this? -->

## Tables

All tables created within BlueBase are gathered on the **Tables** page. New tables can be created and existing tables can be updated or deleted here.

### Create new table

To create a new table, click **+ New table** on the **Tables** page.
Tables can be created from scratch or from a template that was previously saved.
Once a table is saved it is no longer possible to edit the schema, only new fields can be added. To edit an existing schema, the original schema can be copied as text and pasted into a new empty table where the necessary changes can be made before saving this new table.

#### Empty Table

To create a table from scratch, complete all fields as indicated below in the following sections and click the **Save** button. Once saved, a job will be created to create the table. To view table creation progress, navigate to the **Activity** page.

##### Table information

The table name is a required field and must be a unique name. The first character of the table must be a letter followed by letters, numbers or underscores. The description is optional.

##### References

Including or excluding references can be done by checking or un-checking the **Include reference** checkbox. By including references, additional columns will be added to the schema (see next paragraph) which can contain references to the data on the platform:

* **data_reference**: reference to the data element in the Illumina platform from which the record originates
* **data_name**: original name of the data element in the Illumina platform from which the record originates
* **sample_reference**: reference to the sample in the Illumina platform from which the record originates
* **sample_name**: name of the sample in the Illumina platform from which the record originates
* **pipeline_reference**: reference to the pipeline in the Illumina platform from which the record originates
* **pipeline_name**: name of the pipeline in the Illumina platform from which the record originates
* **execution_reference**: reference to the pipeline execution in the Illumina platform from which the record originates
* **account_reference**: reference to the account in the Illumina platform from which the record originates
* **account_name**: name of the account in the Illumina platform from which the record originates

##### Schema

In an empty table, users can create a schema by adding a field for each column of the table and defining it. The **+ Add field** button can be found in the upper, right-hand corner of the schema.
At any time during the creation process it is possible to switch to the ‘edit as text’ mode and back. The text mode shows the JSON code, whereas the original view shows the fields in a table.

Each field requires:

* a name – this has to be unique
* a type
  * String – collection of characters
  * Bytes – raw binary data
  * Integer – whole numbers
  * Float – fractional numbers
  * Numeric – any number
  * Boolean – only options are “true” or “false”
  * Timestamp - Stores number of (milli)seconds passed since the Unix epoch
  * Date - Stores date in the format YYYY-MM-DD
  * Time - Stores time in the format HH:MI:SS
  * Datetime - Stores date and time information in the format YYYY-MM-DD HH:MI:SS
  * Record – has a child field
* a mode - indicate whether the field value
  * is required
  * can be nullable
  * can be repeated
* it is also possible (but not required) to indicate whether the value of the field matches with a database id from the drop-down list. This will create a hyperlink that will bring you to the database item.

#### From Template

Users can create their own template by making a schema (by creating a new empty table) and clicking “Save as template”.
An overview of all saved table templates in your account can be found on the BlueBase Management page.
<!-- TODO Check for accuracy - @Vicky: Is this still available on the new platform? + which users (permissions) can access this? -->

If a template is created and available/active, it is possible to create a new table based on this template. The table information and references follow the rules of the empty table but in this case the schema will be pre-filled. It is possible to still edit the schema that is based on the template.  

### Table status

The status of a table can be found on the Tables page.
The possible statuses are:

* **Available**: Ready to be used, both with or without data
* **Pending**: The system is still processing the table, there is probably a process running to fill the table with data
* **Deleted**: The table is deleted functionally; it still exists and can be shown in the list again by clicking the “Show deleted tables” button

Additional Considerations

* Tables created from empty data or from a template are “Available” faster.
* When copying a table with data, it takes longer. These can remain in a “Pending” state for longer periods of time.
* Clicking on the page's refresh button will update the list.

### Table details

For any available table, the following details can be found:

* **Table information**: Name, description, number of records and data size
* **Schema definition**: An overview of the table schema, also available in text. Fields can be added to the schema but not deleted. For deleting fields: copy the schema as text and paste in a new empty table where the schema is still editable.
* **Preview**: A preview of the table for the 50 first rows (when data is uploaded into the table)
* **Data**: the files that are currently uploaded into the table.

### Table actions

From within the details of a table it is possible to perform the following actions related to the table:

* **Copy**: Create a copy from this table in the same or a different project. In order to copy to another project, data sharing of the original project should be enabled in the details of this project. The user also has to have access to both original and target project.
<!-- TODO: Clarify language around "data sharing". What does it mean to enable it? If this just refers to the acting user being a member of a workgroup that has access to both Projects, what role must the Workgroup have in each project to accomplish the copy action? -->
* **Export as file**: Export this table as a CSV, JSON or PARQUET file. The exported file can be found in a project where the user has the access to download it.
* **Save as template**: Save the schema or an edited form of it as a template.
* **Add data**: Load additional data into the table manually. This can be done by selecting data files previously uploaded to the project, or by dragging and dropping files directly into the popup window for adding data to the table. It’s also possible to load data into a table manually or automatically via a pre-configured job. This can be done on the **Schedule** page.
* **Delete**: Delete the table.

## Query

Queries can be used for data mining. On the **Query** page:

* New queries can be created and executed
* Already executed queries can be found in the query history
* Saved queries and query templates are listed under the saved queries tab.

### New Query

#### Available tables

All available tables and their details are listed on the **New Query** tab. There are 3 types of tables that can be used for querying:

* **Created tables**: Tables created in the project by a user.
* **Metadata tables**: Contains metadata from samples that are linked to a table in BlueBase. It is created by syncing it with the BlueBase module. This synchronization is configured on the **Details** page within the project.
* **Public tables**: public databases that are made available within BlueBase by Illumina
<!-- TODO Review for accuracy - from Vicky: Is this functionality present in the new platform? Verify both for Metadata and Public tables -->

#### Create new query

##### Input

Queries are executed using standard SQL (e.g.,  Select * From table).
While running the query, errors are checked. If errors are captured, this is represented above the input box.
The query can be immediately executed or saved for future use.

##### Result

If the query is valid for execution, the result will be shown as a table underneath the input box.
From within the result page of the query, it is possible to save the result in two ways:

* **Download**: As Excel or JSON file to the computer.
* **Export**: As new table, as a view or as file to the project in CSV, JSON or AVRO format.

### Query history

The query history lists all queries that were executed. Historical queries are shown with their date, executing user, returned rows and duration of the run.
For each historical query listed, it is possible to:

* **Open**: This will open the query again in the “New query” tab.
* **Save**: This will save the query, so it will be visible in the “Saved queries” tab.
* **View results**: The results of an executed query are available for approximately 24 hours. To see the results after that time period, the query needs to be re-executed.

### Saved queries

All queries saved within the project are listed under the “Saved Queries” tab together with the query templates.

The saved queries can be:

* **Opened**: This will open the query again in the “New query” tab.
* **Saved as template**: The saved query becomes a query template.
* **Deleted**: The query is removed from the list and cannot be opened again.

The query templates can be:

* **Opened**: This will open the query again in the “New query” tab.
* **Deleted**: The query is removed from the list and cannot be opened again.

It is possible to edit the saved queries and templates by double-clicking on each query or template. The data classification of the templates can also be changed as follows:

* **Account**: The query template will be available for everyone within the account
* **User**: The query template will be available for the user who created it

## Schedule

On the **Schedule** page within the BlueBase module, it’s possible to create a job for importing different types of data you have access to into an existing table.
You can schedule this job to run automatically and/or have it executed on demand:

* **Automatic import**: Check the **Active** box within the configured schedule. The job will run on a daily basis.
<!-- TODO Review for accuracy - from Vicky: The turnaround time for running this job depend on what has been configured on the platform. I don’t know what has been configured. -->
* **Manual import**: Select the schedule to run and click the **Run** button.

### Configure a schedule

There are three types of schedules that can be set up.

#### Files

This type will load the content of specific files from this project into a table.
When adding or editing this schedule you can define the following parameters:

* **Active**: The job will run automatically if checked
* **Name – required field**: The name of the scheduled job
* **Description**: Extra information about the schedule
* **Source**:
  * Project: All files with the correct naming from this project will be used.
* **Search for a part of a specific ‘Original Name’ or Tag – required field**: Define in this field a part or the full name of the file name or of the tag that the files you want to upload contain. For example, if you want to import files named sample1\_reads.txt, sample2\_reads.txt, … you can fill in \_reads.txt in this field to have all files that contain \_reads.txt imported to the table.
* **Generated by Pipelines**: Only files generated by these selected pipelines are taken into account. When left clear, files from all pipelines are used.
* **Target BlueBase Table – required field**: The table to which the information needs to be added. A drop-down list with all created tables is shown. This means the table needs to be created before the schedule can be created.
* **Write preference**: Define data handling; whether it can overwrite the data
* **Data format - required**: CSV, TSV, JSON, AVRO, PARQUET
* **Delimiter**:  to indicate which delimiter is used in the delimiter separated file. If the delimiter is not present in list, it can be indicated as custom.
* **Custom delimiter**: the custom delimiter that is used in the file.
* **Header rows to skip**: Number of rows in the file that can be skipped
* **References**: Choose which references must be added to the table
<!-- TODO * **Annotation fields**: -->

<!-- 
EXCLUDED AS THIS FEATURE IS CURRENTLY NOT SUPPORTED
@Gert Willems: metadata doesn’t work currently with SnowFlake. Remove this from the current documentation?

5.1.2	Metadata
This type will load metadata (added in the samples) into a newly created table. The process gathers the metadata from the samples via the data linked to the project and the metadata from the runs in this project. 
When adding or editing this schedule you can define the following parameters:
•	Active: the job will run automatic if ticked
•	Name – required field: the name of this scheduled job
•	Description: Extra information about the schedule
•	Source:
o	Project: all meta data from this project will be added
o	Account: all meta data from every project in the account will be added
•	Anonymize references: when ticked, the references will not be added
•	Include sensitive meta data fields: in the meta data fields configuration, fields can be set to sensitive. When checked, those fields will also be added.
-->

#### Administrative data

This type will automatically create a table and load administrative data into this table. A usage overview of all executions is considered administrative data.

When adding or editing this schedule the following parameters can be defined:

* **Active**: The job will run automatically if checked
* **Name – required field**: The name of this scheduled job
* **Description**: Extra information about the schedule
* **Source**:
  * Project: All administrative data from this project will be added
  * Account: All administrative data from every project in the account will be added
<!-- Need to confirm permissions on this feature. Is this only for the tenant admin? or based on the acting user's role within each project to which they have access? -->
* **Anonymize references**: When checked, any platform references will not be added
* **Include sensitive metadata fields**: In the metadata fields configuration, fields can be set to sensitive. When checked, those fields will also be added.

### Delete schedule

Schedules can be deleted. Once deleted, they will no longer run, and they will not be shown in the list of schedules.

### Run schedule

When clicking the **Run** button, the schedule will start the job of importing the configured data in the correct tables. This way the schedule can be run manually.
The result of the job can be seen in the tables.

## Activity

In the **Activity** menu, “BlueBase jobs” and “BlueBase activity” can be found.

### BlueBase jobs

The **Jobs** page gives an overview of all the actions related to a table or a query that have run or are running (e.g., Copy table, export table, Select * from table, etc.)

The jobs are shown with their:

* Creation time: When did the job start
* Description: The query or the performed action with some extra information
* Type: Which action was taken
* Status: Failed or succeeded
* Duration: How long the job took
* Billed bytes: The used bytes that need to be payed

Failed jobs provide information on why the job failed. Details are accessed by double-clicking the failed job. Jobs in progress can be aborted here.

### BlueBase Activity

The **Activity** page gives an overview of previous results (e.g., Executed query, Succeeded Exporting table, Created table, etc.)

The activities are shown with:

* Date: The moment the action was done
* User: The user that requested the action
* Description: An explanation of the action

This list can be exported to excel.

<!--
EXCLUDED AS THE FEATURE IS NOT CURRENTLY SUPPORTED

7	BlueBase Mgmt
This is accessible on account level, not in a project.
7.1	Table templates
Table templates are the templates that can be used to create a new table.
On this page all templates of schemas that were created in the account are listed. They are shown with their name, active status, description, data classification and the owning account.
From here, templates can be created or deleted.
Only the templates that are active will be shown in the dropdownlist when creating a new table from a template.
7.1.1	Create template
The creation of a template here happens in the same way as when going via creating a new table.
You have to give the template a name and add fields to create a schema.
7.1.2	Delete template
When deleting a template, it is no longer shown in the list of templates and it can no longer be chosen when creating a new table.
7.2	Query templates
Query templates can be used to perform a new query and not having to start from scratch.
On this page all templates of queries that were created in the account are listed. They are shown with their name, description, data classification and the owning account.
From here, queries can be created or deleted.
7.2.1	Create template
To create a template, fill out following fields in the pop-up:
•	Name – required field: the name you give the query template to recognize it
•	Description: an explanation about the query
•	Data classification – required field:
o	Account: everyone in the account can see and use it
o	User: only the user can see and use it
o	Public: everyone in every account can see this
•	Query: here you write the query itself, eg. Select * from table
7.2.2	Delete template
When deleting a template, it is no longer shown in the list of templates here nor in the list of “Saved queries” in the queries menu.
7.3	Upload formats
The upload formats are the different ways files can be uploaded.
On this page all different types of formats that can be used in the account are listed. They are shown with their name, description, data classification and the owning account.
From here, upload formats can be created or deleted.
7.3.1	Create upload format
To create an upload format, fill out following fields in the pop-up:
•	Active: if checked, this type can be used
•	Name – required field: the name you give the upload format to recognize it
•	Description: an explanation about the upload format
•	Big query/snowflake: when checked, the upload format can be used for that database
•	Data classification – required field:
o	Account: everyone in the account can see and use it
o	User: only the user can see and use it
o	Public: everyone in every account can see this
•	Prepare script: ??
•	Load script: ??
7.3.2	Delete upload format
When deleting an upload format, it is no longer shown in the list here and it can no longer be used as a way to upload files.
7.4	Datawarehouse sizes <@Vicky: I think this section is incomplete due to an incomplete GUI at this moment as when testing this it gives me errors that there are a lot of values not provided>
The datawarehouse sizes are the different sizes the warehouses where the data is stored can have.
On this page all different sizes that can be used in the account are listed. They are shown with their name, data classification and the owning account.
From here, datawarehouse sizes can be created or deleted.
7.4.1	Create datawarehouse sizes
To create a datawarehouse size, fill out following fields in the pop-up:
•	Active: if checked, this type can be used
•	Name – required field: the name you give the datawarehouse size to recognize it
•	Data classification – required field:
o	Account: everyone in the account can see and use it
o	User: only the user can see and use it
o	Public: everyone in every account can see this
7.4.2	Delete datawarehouse sizes
When deleting a datawarehouse size, it is no longer shown in the list here and it can no longer be used as a size when creating an entitlement.

-->
